# ConvLSTM Configuration: Upstream Experiment (12GB GPU)
# This configuration trains a model using both upstream and downstream region data
# Optimized for 12GB GPU memory

# Data Configuration
data:
  data_path: "data/regional_weather.nc"
  output_dir: "checkpoints/with_upstream"
  
  # Data splitting (temporal ordering preserved)
  train_ratio: 0.7
  val_ratio: 0.15
  # Remaining 0.15 for test set
  
  # Window configuration
  window_size: 6  # 3 days at 12-hour intervals
  target_offset: 1  # Predict 12 hours ahead

# Region Configuration
region:
  # Downstream region (target prediction area)
  downstream:
    lat_min: 25.0
    lat_max: 40.0
    lon_min: 110.0
    lon_max: 125.0
  
  # Upstream region (influencing area west of downstream)
  upstream:
    lat_min: 25.0
    lat_max: 40.0
    lon_min: 70.0
    lon_max: 110.0
  
  include_upstream: true  # Enable upstream region

# Model Architecture
model:
  input_channels: 56  # 5 vars Ã— 11 levels + 1 precip
  hidden_channels: [32, 64]  # Encoder and bottleneck hidden dims
  kernel_size: 3
  output_channels: 1  # Precipitation only

# Training Configuration
training:
  # Optimization
  learning_rate: 0.001
  weight_decay: 0.00001
  batch_size: 4  # May need to reduce to 2-3 due to larger input
  num_epochs: 100
  
  # Gradient management
  gradient_clip_norm: 1.0
  gradient_accumulation_steps: 1  # Increase to 2 if OOM
  
  # Memory optimization
  use_amp: true  # Mixed precision training (essential)
  num_workers: 2  # DataLoader workers
  pin_memory: true
  
  # Scheduler
  scheduler_type: "cosine"  # cosine, step, or none
  scheduler_patience: 5  # For ReduceLROnPlateau
  scheduler_factor: 0.5

# Loss Function Configuration
loss:
  high_precip_threshold: 10.0  # mm
  high_precip_weight: 3.0  # Weight multiplier for high precipitation
  latitude_weighting: true  # Apply cos(latitude) weighting
  # Loss is computed only on downstream region (masking applied automatically)

# Checkpointing and Validation
checkpointing:
  checkpoint_frequency: 1000  # Save every N steps
  validation_frequency: 500  # Validate every N steps
  early_stopping_patience: 10  # Stop if no improvement for N validations
  save_best_only: false  # Also save periodic checkpoints

# Logging
logging:
  log_level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  log_to_file: true
  log_to_console: true

# Device Configuration
device:
  device: "auto"  # auto, cuda, cpu
  cuda_device: 0  # GPU index if multiple GPUs available

# Expected Performance
# - Peak memory: ~5-7 GB (larger input due to upstream region)
# - Training time: ~40-60 minutes per epoch
# - Total training: ~65-100 hours for 100 epochs
# 
# Note: If OOM occurs, reduce batch_size to 2 and increase 
# gradient_accumulation_steps to 2 to maintain effective batch size of 4
